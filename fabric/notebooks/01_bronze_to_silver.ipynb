# Fabric Notebook: 01_Bronze_to_Silver
# Description: Ingest raw data, clean, deduplicate, and merge into Silver Delta tables.

from pyspark.sql.functions import col, current_timestamp, to_date
from delta.tables import DeltaTable

# 1. Configuration
bronze_path = "Files/Bronze/sap_ecc"
silver_table_name = "silver_customer"
silver_path = "Tables/Silver/silver_customer"

# 2. Read Bronze Data (Raw)
# In Fabric check for new files (Pattern: 2024/05/10 /*.json)
df_bronze = spark.read.format("json").load(f"{bronze_path}/*/*/*.json")

# 3. Transformations & Cleaning
df_cleaned = df_bronze.select(
    col("KUNNR").alias("customer_id"),
    col("NAME1").alias("name"),
    col("LAND1").alias("country"),
    col("ORT01").alias("region"),
    col("BRSCH").alias("industry"),
    col("SOURCE_SYS").alias("source_system"),
    current_timestamp().alias("ingestion_timestamp")
).dropDuplicates(["customer_id"])

# 4. Schema Drift Handling
# Enable mergeSchema to allow evolution if new columns satisfy rules
spark.conf.set("spark.databricks.delta.schema.autoMerge.enabled", "true")

# 5. Merge into Silver (Upsert)
if not DeltaTable.isDeltaTable(spark, silver_path):
    # Initial Load
    df_cleaned.write.format("delta").mode("overwrite").save(silver_path)
    spark.sql(f"CREATE TABLE IF NOT EXISTS {silver_table_name} USING DELTA LOCATION '{silver_path}'")
else:
    # Incremental Merge
    delta_table = DeltaTable.forPath(spark, silver_path)
    delta_table.alias("tgt").merge(
        df_cleaned.alias("src"),
        "tgt.customer_id = src.customer_id"
    ).whenMatchedUpdateAll(
    ).whenNotMatchedInsertAll(
    ).execute()

# 6. Optimization (Fabric Specific)
# V-Order is enabled by default in Fabric, but explicit OPTIMIZE helps with compaction
spark.sql(f"OPTIMIZE {silver_table_name} VORDER")
