# Fabric Notebook: 02_Silver_to_Gold
# Description: aggregated business logic for Reporting (Star Schema)

from pyspark.sql.functions import col, sum, count, month, year, lit
from delta.tables import *

# 1. Load Silver Tables
df_silver_sales = spark.read.format("delta").table("silver_sales")
df_silver_cust = spark.read.format("delta").table("silver_customer")

# 2. Compute Fact Table: fact_sales
# Join with dimensions to get Surrogate Keys (simulated here)
df_fact = df_silver_sales.join(
    df_silver_cust,
    df_silver_sales.customer_id == df_silver_cust.customer_id,
"left"
).select(
    col("transaction_id").alias("sales_key"), # In real scenario, use monotonic_id or hash
    col("customer_id").alias("customer_key"), # Should map to Dim Surrogate Key
    col("amount"),
    col("transaction_date")
)

# 3. Write Fact Table (Gold)
fact_table_name = "fact_sales"
fact_path = "Tables/Gold/fact_sales"

df_fact.write.format("delta").mode("overwrite").option("overwriteSchema",
"true").save(fact_path)
spark.sql(f"CREATE TABLE IF NOT EXISTS {fact_table_name} USING DELTA LOCATION '{fact_path}'")

# 4. Compute Aggergates (Optional Materialized View logic)
df_agg = df_fact.groupBy("customer_key").agg(
    sum("amount").alias("total_revenue"),
    count("sales_key").alias("transaction_count")
)

df_agg.write.format("delta").mode("overwrite").save("Tables/Gold/agg_customer_sales")

# 5. Optimization for DirectLake
# Z-Order by high cardinality columns used in filters
spark.sql(f"OPTIMIZE {fact_table_name} VORDER ZORDER BY (transaction_date)")
